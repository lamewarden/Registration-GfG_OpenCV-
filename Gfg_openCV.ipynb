{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import block\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity transformation \n",
    "## Working principle of the method\n",
    "\n",
    "The OpenCV library provides a way to calculate a similarity transformation matrix, which is a 2x3 affine matrix with four degrees of freedom: scale, rotation, x translation, and y translation. This matrix can be used to align images taken in any given channel with a template image (which is by default in channel 4).\n",
    "\n",
    "Once the similarity transformation matrix is calculated, it can be applied to the image of the respective channel. This ensures that all images are perfectly aligned with channel 4. The process is fast, with the initial calibration step taking around 6 seconds, and applying the matrix transformation for one channel taking around 19 milliseconds. If you apply the transformation for all channels at once, it takes around 60 milliseconds.\n",
    "\n",
    "Because the process is quick, you can potentially conduct a full \"calibration\" every time the camera software is initiated.\n",
    "\n",
    "ANd here my first question arises: how do you apply the configuration from the corrections.xml file? Would it be possible (or easier) to just apply the linear transformation for each channel, rather than seeking exact pixel, scale, and rotation angle volumes?\n",
    "\n",
    "If it's not possible (or not easier), you can get these values from the matrix by decomposition. The similarity matrix can be represented as:\n",
    "\n",
    "\\begin{bmatrix} \\cos(\\theta) \\cdot s & -\\sin(\\theta) \\cdot s & t_x \\\\ \\sin(\\theta) \\cdot s & \\cos(\\theta) \\cdot s & t_y \\end{bmatrix}\n",
    "\n",
    "where t_x and t_y are translation (shift) values in pixels, s is the scale, and \\theta is the rotation angle.\n",
    "\n",
    "While the scale factor and rotation angle fit perfectly well with your corrections.xml file, the shift values are always off. I am confident that the similarity matrix is calculated correctly because when I decompose it into separate scale, rotation, and translation matrices and then apply them consecutively to the image, I get the image perfectly aligned with channel 4. This brings us back to the question of how to apply the configuration from the corrections.xml file. In this particular case, the order of transformations is important. The scale should be applied first, then the rotation, and finally the translation. If the order is changed, you will get different results, which could be the reason why our x and y shift values don't match.\n",
    "\n",
    "You can see how this works in the code below. Input images are taken from the camera, but you can use any images you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def channels_similarity_transform(root_dir=('./Input_images/'), template_img=0):\n",
    "    ''' This function takes a directory with images and a number of template image \n",
    "    and returns a list of similarity matrices for each channel respectively to the template image'''\n",
    "    # Set the directory\n",
    "    root_dir=Path(root_dir)\n",
    "    # Read the images into a list\n",
    "    images = [cv2.imread(str(root_dir / f)) for f in os.listdir(root_dir)]\n",
    "    # Convert the images to grayscale\n",
    "    gray_images = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]\n",
    "    # Register all images to the first image\n",
    "    template = gray_images[template_img]\n",
    "\n",
    "    height, width = template.shape\n",
    "\n",
    "    # Create ORB detector with 5000 features.\n",
    "    orb_detector = cv2.ORB_create(5000)\n",
    "\n",
    "    # Find keypoints and descriptors.\n",
    "    # The first arg is the image, second arg is the mask\n",
    "    #  (which is not required in this case).\n",
    "    # here just for template image\n",
    "    k_temp, d_temp = orb_detector.detectAndCompute(template, None)\n",
    "    # initiating a list of all SG matrices\n",
    "    SG_arr = []\n",
    "    # iterating through the gray images.\n",
    "    for num, image in enumerate(gray_images):\n",
    "        k_ch, d_ch = orb_detector.detectAndCompute(image, None)\n",
    "\n",
    "        # Match features between the two images.\n",
    "        # We create a Brute Force matcher with\n",
    "        # Hamming distance as measurement mode.\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        # Match the two sets of descriptors.\n",
    "        matches = list(matcher.match(d_temp, d_ch))\n",
    "\n",
    "        # Sort matches on the basis of their Hamming distance.\n",
    "        matches.sort(key=lambda x: x.distance)\n",
    "\n",
    "        # Take the top 90 % matches forward.\n",
    "        matches = matches[:int(len(matches)*0.9)]\n",
    "        no_of_matches = len(matches)\n",
    "\n",
    "        # Define empty matrices of shape no_of_matches * 2.\n",
    "        pt = np.zeros((no_of_matches, 2))\n",
    "        p_ch = np.zeros((no_of_matches, 2))\n",
    "\n",
    "        for i in range(len(matches)):\n",
    "            pt[i, :] = k_temp[matches[i].queryIdx].pt\n",
    "            p_ch[i, :] = k_ch[matches[i].trainIdx].pt\n",
    "\n",
    "        # Find the SG matrix.\n",
    "        affine_transform, mask = cv2.estimateAffinePartial2D(p_ch, pt, cv2.RANSAC)\n",
    "        SG_arr.append(affine_transform)\n",
    "        # Use this matrix to transform the\n",
    "        # colored image wrt the reference image.\n",
    "        transformed_img = cv2.warpAffine(images[num], affine_transform, (width, height))\n",
    "\n",
    "        # Save the output.\n",
    "        cv2.imwrite(f'./outputs/output_channel{num}.png', transformed_img)\n",
    "    # now we save homography_arr as csv file\n",
    "\n",
    "    b = open('SG_matrices.csv', 'w')\n",
    "    a = csv.writer(b)\n",
    "    a.writerows(SG_arr)\n",
    "    b.close()\n",
    "    return SG_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 735 ms, total: 19.8 s\n",
      "Wall time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matrices_arr = channels_similarity_transform(template_img=4)\n",
    "# adjusted files are saved into the outputs folder, \n",
    "# Similarity matrices are saved into the atrices_arr variable and the csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix decomposition\n",
    "Ideal solution would be to transpose every channel image by the given similarity matrix. The operation itself is quick and can be conducted on the go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep steps (made once during the initial calibration)\n",
    "img_1 = cv2.imread('./Input_images/1st alignment_ch0.png')\n",
    "matrices_arr = channels_similarity_transform(template_img=4)\n",
    "height, width, chan = img_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 104 ms, sys: 27 ms, total: 131 ms\n",
      "Wall time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# transformation step (made for each channel)\n",
    "transformed_img = cv2.warpAffine(1, matrices_arr[1], (width, height))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity matrix decomposition\n",
    "However, if not possible (or not desired) we can decompose similarity matrix to Scale factor, rotation angle and translation values from the formula:\n",
    "\n",
    "\\begin{bmatrix} \\cos(\\theta) \\cdot s & -\\sin(\\theta) \\cdot s & t_x \\\\ \\sin(\\theta) \\cdot s & \\cos(\\theta) \\cdot s & t_y \\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_decompose(list_of_arrs):\n",
    "    ''' This function decomposes the similarity matrix into scale coeficient, rotation degree and pixel shift'''\n",
    "    for num, array in enumerate(list_of_arrs):\n",
    "        tn_teta = round(\n",
    "            round(array[1, 0], 6)/round(array[0, 0], 6), 6)\n",
    "        teta_degrees = round(np.arctan(tn_teta)*180/np.pi, 6)\n",
    "        scaling = round(array[0, 0]/np.cos(teta_degrees*np.pi/180), 6)\n",
    "        print(f\"------------channel {num}-----------------\")\n",
    "        print(f\"Pixel shift for channel {num} in x is: {round(array[0,2],2)}\")\n",
    "        print(f\"Pixel shift for channel {num} in y is: {round(array[1,2],2)}\")\n",
    "        print(f\"Rotation for channel {num} is {round(teta_degrees, 4)} degrees: \")\n",
    "        print(f\"Scale coeficient for channel {num} is {scaling}\")\n",
    "        print(f\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While scale and rotation correspond perfectly well, x and y shifts fo not fit with your corrections.xml. That's why I wonder in what order you apply the transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------channel 0-----------------\n",
      "Pixel shift for channel 0 in x is: -2.47\n",
      "Pixel shift for channel 0 in y is: 7.98\n",
      "Rotation for channel 0 is -0.2668 degrees: \n",
      "Scale coeficient for channel 0 is 0.998375\n",
      "\n",
      "------------channel 1-----------------\n",
      "Pixel shift for channel 1 in x is: 2.84\n",
      "Pixel shift for channel 1 in y is: 2.85\n",
      "Rotation for channel 1 is -0.1291 degrees: \n",
      "Scale coeficient for channel 1 is 0.995348\n",
      "\n",
      "------------channel 2-----------------\n",
      "Pixel shift for channel 2 in x is: 1.13\n",
      "Pixel shift for channel 2 in y is: -0.52\n",
      "Rotation for channel 2 is 0.0828 degrees: \n",
      "Scale coeficient for channel 2 is 0.99945\n",
      "\n",
      "------------channel 3-----------------\n",
      "Pixel shift for channel 3 in x is: -5.08\n",
      "Pixel shift for channel 3 in y is: -0.03\n",
      "Rotation for channel 3 is -0.1976 degrees: \n",
      "Scale coeficient for channel 3 is 1.002184\n",
      "\n",
      "------------channel 4-----------------\n",
      "Pixel shift for channel 4 in x is: 0.0\n",
      "Pixel shift for channel 4 in y is: 0.0\n",
      "Rotation for channel 4 is 0.0 degrees: \n",
      "Scale coeficient for channel 4 is 1.0\n",
      "\n",
      "------------channel 5-----------------\n",
      "Pixel shift for channel 5 in x is: -16.07\n",
      "Pixel shift for channel 5 in y is: 11.41\n",
      "Rotation for channel 5 is -0.7373 degrees: \n",
      "Scale coeficient for channel 5 is 1.00186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrices_arr = channels_similarity_transform(template_img=4)\n",
    "similarity_decompose(matrices_arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some other methods were checked below\n",
    "All working, but homography and classic 6DOF affine transformations, but they seems like excessive, in this particular case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homography matrix (2 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Open the image files.\n",
    "# Image to be aligned.\n",
    "img1_color = cv2.imread(\n",
    "    \"/mnt/buf/PSI/Multispec/2023.04.21_alignment_photo/Original images/1st alignment_ch0.png\")\n",
    "# Reference image.\n",
    "img2_color = cv2.imread(\n",
    "    \"/mnt/buf/PSI/Multispec/2023.04.21_alignment_photo/Original images/1st alignment_ch5.png\")\n",
    "\n",
    "# Convert to grayscale.\n",
    "img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY)\n",
    "height, width = img2.shape\n",
    "\n",
    "# Create ORB detector with 5000 features.\n",
    "orb_detector = cv2.ORB_create(5000)\n",
    "\n",
    "# Find keypoints and descriptors.\n",
    "# The first arg is the image, second arg is the mask\n",
    "#  (which is not required in this case).\n",
    "kp1, d1 = orb_detector.detectAndCompute(img1, None)\n",
    "kp2, d2 = orb_detector.detectAndCompute(img2, None)\n",
    "\n",
    "# Match features between the two images.\n",
    "# We create a Brute Force matcher with\n",
    "# Hamming distance as measurement mode.\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match the two sets of descriptors.\n",
    "matches = list(matcher.match(d1, d2))\n",
    "\n",
    "# Sort matches on the basis of their Hamming distance.\n",
    "matches.sort(key=lambda x: x.distance)\n",
    "\n",
    "# Take the top 90 % matches forward.\n",
    "matches = matches[:int(len(matches)*0.9)]\n",
    "no_of_matches = len(matches)\n",
    "\n",
    "# Define empty matrices of shape no_of_matches * 2.\n",
    "p1 = np.zeros((no_of_matches, 2))\n",
    "p2 = np.zeros((no_of_matches, 2))\n",
    "\n",
    "for i in range(len(matches)):\n",
    "    p1[i, :] = kp1[matches[i].queryIdx].pt\n",
    "    p2[i, :] = kp2[matches[i].trainIdx].pt\n",
    "\n",
    "# Find the homography matrix.\n",
    "homography, mask = cv2.findHomography(p1, p2, cv2.RANSAC)\n",
    "\n",
    "% % time\n",
    "# Use this matrix to transform the\n",
    "# colored image wrt the reference image.\n",
    "transformed_img = cv2.warpPerspective(img1_color,\n",
    "                                      homography, (width, height))\n",
    "\n",
    "# Save the output.\n",
    "cv2.imwrite('output.png', transformed_img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All images in the folder (homography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_images(root_dir=Path('/mnt/buf/PSI/Multispec/Code/Registration(Medium)/FIRE/Images'), template_img=0):\n",
    "\n",
    "    # Get the list of image filenames (I left only those, which apparently represented same eye)\n",
    "    image_filenames = [f for f in os.listdir(root_dir)]\n",
    "\n",
    "    # Read the images into a list\n",
    "    images = [cv2.imread(str(root_dir / f)) for f in image_filenames]\n",
    "    # Convert the images to grayscale\n",
    "    gray_images = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]\n",
    "    # Register all images to the first image\n",
    "    template = gray_images[template_img]\n",
    "\n",
    "    height, width = template.shape\n",
    "\n",
    "    # Create ORB detector with 5000 features.\n",
    "    orb_detector = cv2.ORB_create(5000)\n",
    "\n",
    "    # Find keypoints and descriptors.\n",
    "    # The first arg is the image, second arg is the mask\n",
    "    #  (which is not required in this case).\n",
    "    kt, dt = orb_detector.detectAndCompute(template, None)\n",
    "    homography_arr = []\n",
    "    for num, image in enumerate(gray_images):\n",
    "        k2, d2 = orb_detector.detectAndCompute(image, None)\n",
    "\n",
    "        # Match features between the two images.\n",
    "        # We create a Brute Force matcher with\n",
    "        # Hamming distance as measurement mode.\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "        # Match the two sets of descriptors.\n",
    "        matches = list(matcher.match(dt, d2))\n",
    "\n",
    "        # Sort matches on the basis of their Hamming distance.\n",
    "        matches.sort(key=lambda x: x.distance)\n",
    "\n",
    "        # Take the top 90 % matches forward.\n",
    "        matches = matches[:int(len(matches)*0.9)]\n",
    "        no_of_matches = len(matches)\n",
    "\n",
    "        # Define empty matrices of shape no_of_matches * 2.\n",
    "        pt = np.zeros((no_of_matches, 2))\n",
    "        p2 = np.zeros((no_of_matches, 2))\n",
    "\n",
    "        for i in range(len(matches)):\n",
    "            pt[i, :] = kt[matches[i].queryIdx].pt\n",
    "            p2[i, :] = k2[matches[i].trainIdx].pt\n",
    "\n",
    "        # Find the homography matrix.\n",
    "        homography, mask = cv2.findHomography(p2, pt, cv2.RANSAC)\n",
    "        homography_arr.append(homography)\n",
    "        # Use this matrix to transform the\n",
    "        # colored image wrt the reference image.\n",
    "        transformed_img = cv2.warpPerspective(images[num],\n",
    "                                              homography, (width, height))\n",
    "\n",
    "        # Save the output.\n",
    "        cv2.imwrite(f'output_channel{num}.png', transformed_img)\n",
    "    # now we save homography_arr as csv file\n",
    "\n",
    "    b = open('homography_matrices.csv', 'w')\n",
    "    a = csv.writer(b)\n",
    "    a.writerows(homography_arr)\n",
    "    b.close()\n",
    "    b\n",
    "    return homography_arr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homography matrix decomposition (so far failing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_homography(H):\n",
    "    # Extract rotation and translation\n",
    "    R = H[:2, :2]\n",
    "    t = H[:2, 2]\n",
    "\n",
    "    # Calculate rotation angle in degrees\n",
    "    theta = np.arctan2(R[1, 0], R[0, 0]) * 180 / np.pi\n",
    "\n",
    "    # Calculate pixel shift in x and y directions\n",
    "    shift_x = t[0]\n",
    "    shift_y = t[1]\n",
    "    # Calculate scaling factor\n",
    "    s = np.linalg.norm(R[:, 0]) + np.linalg.norm(R[:, 1]) / 2\n",
    "\n",
    "    return shift_x, shift_y, theta, s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%` not found.\n"
     ]
    }
   ],
   "source": [
    "%% time\n",
    "hom_arr = retrieve_images(template_img=4)\n",
    "for num, arr in enumerate(hom_arr):\n",
    "    # Example usage\n",
    "    shift_x, shift_y, theta, s = decompose_homography(arr)\n",
    "    print(f\"--------------Channel {num}---------------\")\n",
    "    print(f\"Pixel shift in x of channel {num} is: {round(shift_x,2)}\")\n",
    "    print(f\"Pixel shift in y of channel {num} is: {round(shift_y, 2)}\")\n",
    "    print(f\"Rotation in channel {num} is {round(theta, 2)} degrees: \")\n",
    "    print(f\" \")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine transformation variant\n",
    "(should be a bit faster, but little bit less precise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the image files.\n",
    "# Image to be aligned.\n",
    "img1_color = cv2.imread(\n",
    "    \"/mnt/buf/PSI/Multispec/2023.04.21_alignment_photo/Original images/1st alignment_ch0.png\")\n",
    "# Reference image.\n",
    "img2_color = cv2.imread(\n",
    "    \"/mnt/buf/PSI/Multispec/2023.04.21_alignment_photo/Original images/1st alignment_ch5.png\")\n",
    "\n",
    "# Convert to grayscale.\n",
    "img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY)\n",
    "height, width = img2.shape\n",
    "\n",
    "# Create ORB detector with 5000 features.\n",
    "orb_detector = cv2.ORB_create(5000)\n",
    "\n",
    "# Find keypoints and descriptors.\n",
    "kp1, d1 = orb_detector.detectAndCompute(img1, None)\n",
    "kp2, d2 = orb_detector.detectAndCompute(img2, None)\n",
    "\n",
    "# Match features between the two images.\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = list(matcher.match(d1, d2))\n",
    "\n",
    "# Sort matches based on distance.\n",
    "matches.sort(key=lambda x: x.distance)\n",
    "\n",
    "# Take the top 90% matches.\n",
    "matches = matches[:int(len(matches)*0.9)]\n",
    "\n",
    "# Extract matched keypoints.\n",
    "src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Find affine transform.\n",
    "affine, mask = cv2.estimateAffine2D(\n",
    "    src_pts, dst_pts, method=cv2.RANSAC, ransacReprojThreshold=5)\n",
    "\n",
    "# Apply transform to the image.\n",
    "transformed_img = cv2.warpAffine(img1_color, affine, (width, height))\n",
    "\n",
    "# Save the output.\n",
    "cv2.imwrite('Affine_output.png', transformed_img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric similar transformation \n",
    "### (affine with 4 DOF - translation (x; y), rotation, scaling)\n",
    "**Less precise, but easy for decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the image files.\n",
    "# Image to be aligned.\n",
    "img1_color = cv2.imread(\n",
    "    \"/mnt/buf/PSI/Multispec/2023.04.21_alignment_photo/Original images/1st alignment_ch0.png\")\n",
    "# Reference image.\n",
    "img2_color = cv2.imread(\n",
    "    \"/mnt/buf/PSI/Multispec/2023.04.21_alignment_photo/Original images/1st alignment_ch4.png\")\n",
    "\n",
    "# Convert to grayscale.\n",
    "img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY)\n",
    "height, width = img2.shape\n",
    "\n",
    "# Create ORB detector with 5000 features.\n",
    "orb_detector = cv2.ORB_create(5000)\n",
    "\n",
    "# Find keypoints and descriptors.\n",
    "# The first arg is the image, second arg is the mask\n",
    "#  (which is not required in this case).\n",
    "kp1, d1 = orb_detector.detectAndCompute(img1, None)\n",
    "kp2, d2 = orb_detector.detectAndCompute(img2, None)\n",
    "\n",
    "# Match features between the two images.\n",
    "# We create a Brute Force matcher with\n",
    "# Hamming distance as measurement mode.\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match the two sets of descriptors.\n",
    "matches = list(matcher.match(d1, d2))\n",
    "\n",
    "# Sort matches on the basis of their Hamming distance.\n",
    "matches.sort(key=lambda x: x.distance)\n",
    "\n",
    "# Take the top 90 % matches forward.\n",
    "matches = matches[:int(len(matches)*0.9)]\n",
    "no_of_matches = len(matches)\n",
    "\n",
    "# Define empty matrices of shape no_of_matches * 2.\n",
    "p1 = np.zeros((no_of_matches, 2))\n",
    "p2 = np.zeros((no_of_matches, 2))\n",
    "\n",
    "for i in range(len(matches)):\n",
    "    p1[i, :] = kp1[matches[i].queryIdx].pt\n",
    "    p2[i, :] = kp2[matches[i].trainIdx].pt\n",
    "\n",
    "# Find the affine transformation matrix.\n",
    "affine_transform, mask = cv2.estimateAffinePartial2D(p1, p2, cv2.RANSAC)\n",
    "\n",
    "# Use this matrix to transform the\n",
    "# colored image wrt the reference image.\n",
    "transformed_img = cv2.warpAffine(img1_color, affine_transform, (width, height))\n",
    "\n",
    "# Save the output.\n",
    "# cv2.imwrite('output.png', transformed_img)\n",
    "# cv2.imwrite('img1_orig.png', img1)\n",
    "# cv2.imwrite('img2_orig.png', img2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine transform decomposition SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:  [-2.4822889   7.75477429]\n",
      "Rotation:  [[ 0.99844567  0.00460057]\n",
      " [-0.00460057  0.99844567]]\n",
      "Scaling:  [[ 0.          0.99845627]\n",
      " [-0.99845627  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract the translation component\n",
    "translation = affine_transform[:, 2]\n",
    "\n",
    "# Extract the rotation and scaling component\n",
    "rotation_scaling = affine_transform[:, :2]\n",
    "\n",
    "# Compute the scaling component\n",
    "u, s, vh = np.linalg.svd(rotation_scaling)\n",
    "scaling = np.diag(s) @ vh\n",
    "\n",
    "# Print the results\n",
    "print(\"Translation: \", translation)\n",
    "print(\"Rotation: \", rotation_scaling)\n",
    "print(\"Scaling: \", scaling)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "Ask Tom how the adjustment happens in the configuration - first should be scale, then rotation, then translation, not vice versa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99844567,  0.00460057,  0.        ],\n",
       "       [-0.00460057,  0.99844567,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_transform_scale_rot = np.append(affine_transform.T[:2].T, np.array([[0, 0]]).T, axis=1)\n",
    "affine_transform_translation = np.append(np.diag((1, 1)), affine_transform[:,2].reshape((-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_img_trans = cv2.warpAffine(img1_color, affine_transform_translation, (width, height))\n",
    "transformed_img_rot = cv2.warpAffine(transformed_img_trans, affine_transform_scale_rot, (width, height))\n",
    "\n",
    "\n",
    "# Save the output.\n",
    "cv2.imwrite('output_trans_rot.png', transformed_img_trans)\n",
    "cv2.imwrite('img1_orig.png', img1)\n",
    "cv2.imwrite('img2_orig.png', img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SG_decompose(list_of_arrs):\n",
    "    for num, array in enumerate(list_of_arrs):\n",
    "        tn_teta = round(\n",
    "            round(array[1, 0], 6)/round(array[0, 0], 6), 6)\n",
    "        teta_degrees = round(np.arctan(tn_teta)*180/np.pi, 6)\n",
    "        scaling = round(array[0, 0]/np.cos(teta_degrees*np.pi/180), 6)\n",
    "        print(f\"Pixel shift for channel {num} in x is: {round(affine_transform[0,2],2)}\")\n",
    "        print(f\"Pixel shift for channel {num} in y is: {round(affine_transform[1,2],2)}\")\n",
    "        print(f\"Rotation for channel {num} is {round(teta_degrees, 4)} degrees: \")\n",
    "        print(f\"Scale coeficient for channel {num} is {scaling}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
